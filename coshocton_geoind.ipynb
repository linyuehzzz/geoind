{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yue\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\geopandas\\_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "gdf = gpd.read_file('data/coshocton/coshocton_households_all.shp')\n",
    "cnt = int(len(gdf) * 0.01)\n",
    "sample = gdf.sample(n=cnt)\n",
    "sample.to_file('data/coshocton/coshocton_sample.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "gdf1 = gpd.read_file('data/coshocton/coshocton_sample.shp')\n",
    "gdf2 = gpd.read_file('data/coshocton/coshocton_households_all.shp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[27398, 27074, 27396, ..., 27118, 27078, 27391],\n",
       "        [27345, 27401, 27346, ..., 16440, 16434, 27357],\n",
       "        [ 4510,  4509,  4511, ...,  4528,  4067,  4529],\n",
       "        ...,\n",
       "        [10653, 10652, 10651, ..., 10597, 10602, 10596],\n",
       "        [12612, 12610, 12611, ..., 12973, 14516, 12963],\n",
       "        [33571, 33572, 33570, ..., 33485, 33502, 33490]], dtype=int64),\n",
       " array([[   0.        ,   57.91848506,   61.55658089, ...,  155.71473354,\n",
       "          159.3208305 ,  162.32114786],\n",
       "        [   0.        ,   38.40469573,   41.86747929, ...,  172.34433127,\n",
       "          192.69196863,  195.2658188 ],\n",
       "        [   0.        ,   12.26618047,   34.91110445, ...,  645.4459412 ,\n",
       "          675.59638383,  684.78698004],\n",
       "        ...,\n",
       "        [   0.        ,  130.42637498,  148.0035003 , ...,  818.84687802,\n",
       "          831.15955179,  842.30943259],\n",
       "        [   0.        ,   42.38315338,   47.0530484 , ..., 1508.91742983,\n",
       "         1512.94961062, 1517.99917144],\n",
       "        [   0.        ,   18.20265908,   43.95080254, ...,  588.3517563 ,\n",
       "          588.88719906,  589.6576865 ]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "import pickle\n",
    "\n",
    "k_neighbors = 31\n",
    "def get_nearest(src_points, candidates, k_neighbors=10):\n",
    "    \"\"\"\n",
    "    Find nearest neighbors for all source points from a set of candidate points\n",
    "    modified from: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create tree from the candidate points\n",
    "    tree = BallTree(candidates, leaf_size=15, metric='euclidean')\n",
    "\n",
    "    # Find closest points and distances\n",
    "    distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "    # Return indices and distances\n",
    "    return indices, distances\n",
    "\n",
    "in_pts = [(x,y) for x,y in zip(gdf1.geometry.x , gdf1.geometry.y)]\n",
    "qry_pts =  [(x,y) for x,y in zip(gdf2.geometry.x , gdf2.geometry.y)]\n",
    "X, X_dis = get_nearest(in_pts, qry_pts, k_neighbors)\n",
    "pickle.dump(X, open('data/coshocton/coshocton_buff_k' + str(k_neighbors-1) + '.pickle', \"wb\"))\n",
    "pickle.dump(X_dis, open('data/coshocton/coshocton_buff_dis_k' + str(k_neighbors-1) + '.pickle', \"wb\"))\n",
    "\n",
    "X, X_dis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparative analysis with lwized location swapping\n",
    "\n",
    "(1) Expected distance displaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rp_edd(buffer_dis):\n",
    "    edd_rp = []\n",
    "    for row in buffer_dis:\n",
    "        edd_rp.append(np.max(row) / 2)\n",
    "    return sum(edd_rp) / len(edd_rp)\n",
    "\n",
    "def lw_edd(buffer_dis):\n",
    "    edd_lw = []\n",
    "    for row in buffer_dis:\n",
    "        edd_lw.append(np.mean(row))\n",
    "    return sum(edd_lw) / len(edd_lw)\n",
    "\n",
    "def gm_edd(prob, buffer_dis):\n",
    "    edd_gm = []\n",
    "    for idx, row in prob.iterrows():\n",
    "        edd_gm.append(np.dot(buffer_dis[idx], row.tolist()))\n",
    "    return sum(edd_gm) / len(edd_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edd_rp: 206.5233586328766\n",
      "edd_lw: 263.5486984618922\n",
      "edd_gm: 66.32649746050855\n",
      "edd_gm: 93.10535301947102\n",
      "edd_gm: 179.95640918100807\n",
      "edd_gm: 248.20422425604116\n",
      "edd_rp: 314.2902021650227\n",
      "edd_lw: 401.32246384663665\n",
      "edd_gm: 58.70512628146511\n",
      "edd_gm: 103.07682546981354\n",
      "edd_gm: 259.3767437762736\n",
      "edd_gm: 327.22946785066245\n",
      "edd_rp: 387.08124089057895\n",
      "edd_lw: 504.94912204808287\n",
      "edd_gm: 49.67586910561741\n",
      "edd_gm: 110.12731102855918\n",
      "edd_gm: 325.21043940609826\n",
      "edd_gm: 373.32254964125315\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "k_neighbors_all = [10, 20, 30]\n",
    "eps_all = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "with open('data/coshocton/sols/coshocton_edd_all.csv', 'w') as fw:\n",
    "    fw.write('method,eps,k,edd\\n')\n",
    "    fw.flush()\n",
    "\n",
    "    for k_neighbors in k_neighbors_all:\n",
    "        buffer_dis = pickle.load(open('data/coshocton/coshocton_buff_dis_k' + str(k_neighbors) + '.pickle', \"rb\"))\n",
    "        buffer_dis = np.delete(buffer_dis, 0, 1)\n",
    "\n",
    "        # rp\n",
    "        edd_rp = rp_edd(buffer_dis)\n",
    "        fw.write('rp,,' + str(k_neighbors) + ',' + str(edd_rp) + '\\n')\n",
    "        print('edd_rp:', edd_rp)\n",
    "\n",
    "        # lw\n",
    "        edd_lw = lw_edd(buffer_dis)\n",
    "        fw.write('lw,,' + str(k_neighbors) + ',' + str(edd_lw) + '\\n')\n",
    "        print('edd_lw:', edd_lw)\n",
    "        \n",
    "        # gm\n",
    "        for eps in eps_all:\n",
    "            prob = pd.read_csv('data/coshocton/sols/coshocton_prob_eps' + str(eps) + \"_k\" + str(k_neighbors) + '.csv', header=None, index_col=0)\n",
    "            edd_gm = gm_edd(prob, buffer_dis)\n",
    "            fw.write('gm,' + str(eps) + ',' + str(k_neighbors) + ',' + str(edd_gm) + '\\n')\n",
    "            print('edd_gm:', edd_gm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Average nearest neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointpats import PointPattern\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def origin_ann(gdf):\n",
    "    points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "    pp = PointPattern(points)\n",
    "    return pp.mean_nnd\n",
    "\n",
    "\n",
    "def displace_point(point, max_dist):\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    distance = np.random.uniform(0, max_dist)\n",
    "    new_x = point.x + distance * np.cos(angle)\n",
    "    new_y = point.y + distance * np.sin(angle)\n",
    "    return Point(new_x, new_y)\n",
    "\n",
    "def rp_ann(gdf1, buffer_dis, ann_origin, T=100):\n",
    "    ann = []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        for idx, row in gdf1.iterrows():\n",
    "            point = row['geometry']\n",
    "            r = np.max(buffer_dis[idx])\n",
    "            x = displace_point(point, r)\n",
    "            masked_locs.append(x)\n",
    "        gdf = gpd.GeoDataFrame(geometry=masked_locs)\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        pp = PointPattern(points)\n",
    "        ann.append(pp.mean_nnd)\n",
    "    return sum(ann) / len(ann) - ann_origin\n",
    "\n",
    "\n",
    "def lw_ann(gdf2, buffer, ann_origin, T=100):\n",
    "    ann = []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        for locs in buffer:\n",
    "            locs = np.delete(locs, 0)\n",
    "            x = random.choice(locs)\n",
    "            masked_locs.append(x)\n",
    "        gdf = gdf2.iloc[masked_locs]\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        pp = PointPattern(points)\n",
    "        ann.append(pp.mean_nnd)\n",
    "    return sum(ann) / len(ann) - ann_origin\n",
    "\n",
    "\n",
    "def gm_ann(gdf2, prob, buffer, ann_origin, T=100):\n",
    "    ann = []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        for idx, row in prob.iterrows():\n",
    "            locs = buffer[idx]\n",
    "            locs = np.delete(locs, 0)\n",
    "            x = random.choices(locs, weights=tuple(row.tolist()), k=1)[0]\n",
    "            masked_locs.append(x)\n",
    "        gdf = gdf2.iloc[masked_locs]\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        pp = PointPattern(points)\n",
    "        ann.append(pp.mean_nnd)\n",
    "    return sum(ann) / len(ann) - ann_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_origin: 1322.669063757506\n",
      "ann_rp: 10.832201200338432\n",
      "ann_lw: -12.117197866118204\n",
      "ann_gm: -21.81300681706807\n",
      "ann_gm: -7.530490771404402\n",
      "ann_gm: -24.073274622052168\n",
      "ann_gm: -18.861230608462165\n",
      "ann_rp: 19.9293583511801\n",
      "ann_lw: -24.527032274247404\n",
      "ann_gm: -32.94461693889207\n",
      "ann_gm: -8.415925226970785\n",
      "ann_gm: -32.662141409559126\n",
      "ann_gm: -34.202120989113155\n",
      "ann_rp: 28.674887346257037\n",
      "ann_lw: -26.03476990543936\n",
      "ann_gm: -94.0693892811671\n",
      "ann_gm: -12.749812166500305\n",
      "ann_gm: -42.78176428727875\n",
      "ann_gm: -49.1775066790492\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "ann_origin = origin_ann(gdf1)\n",
    "print('ann_origin:', ann_origin)\n",
    "\n",
    "k_neighbors_all = [10, 20, 30]\n",
    "eps_all = [0.1, 0.01, 0.001, 0.0001]\n",
    "T = 100\n",
    "\n",
    "with open('data/coshocton/sols/coshocton_ann_all.csv', 'w') as fw:\n",
    "    fw.write('method,eps,k,ann\\n')\n",
    "    fw.write('origin,,' + str(ann_origin) + '\\n')\n",
    "    fw.flush()\n",
    "    \n",
    "    for k_neighbors in k_neighbors_all:\n",
    "        buffer = pickle.load(open('data/coshocton/coshocton_buff_k' + str(k_neighbors) + '.pickle', \"rb\"))\n",
    "        buffer_dis = pickle.load(open('data/coshocton/coshocton_buff_dis_k' + str(k_neighbors) + '.pickle', \"rb\"))\n",
    "        buffer_dis = np.delete(buffer_dis, 0, 1)\n",
    "        \n",
    "        # rp\n",
    "        ann_rp = rp_ann(gdf1, buffer_dis, ann_origin, T)\n",
    "        fw.write('rp,,' + str(k_neighbors) + ',' + str(ann_rp) + '\\n')\n",
    "        print('ann_rp:', ann_rp)\n",
    "\n",
    "        # lw\n",
    "        ann_lw = lw_ann(gdf2, buffer, ann_origin, T)\n",
    "        fw.write('lw,,' + str(k_neighbors) + ',' + str(ann_lw) + '\\n')\n",
    "        print('ann_lw:', ann_lw)\n",
    "        \n",
    "        # gm\n",
    "        for eps in eps_all:\n",
    "            prob = pd.read_csv('data/coshocton/sols/coshocton_prob_eps' + str(eps) + \"_k\" + str(k_neighbors) + '.csv', header=None, index_col=0)\n",
    "            ann_gm = gm_ann(gdf2, prob, buffer, ann_origin, T)\n",
    "            fw.write('gm,' + str(eps) + ',' + str(k_neighbors) + ',' + str(ann_gm) + '\\n')\n",
    "            print('ann_gm:', ann_gm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Cluster detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 1\n",
      "Estimated number of noise points: 311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "X = np.array(np.stack([gdf1.geometry.x, gdf1.geometry.y], axis=1))\n",
    "db = DBSCAN(eps=1000, min_samples=30).fit(X)\n",
    "labels = db.labels_\n",
    "gdf = gdf1.copy()\n",
    "gdf['db_origin'] = labels\n",
    "gdf.to_file('data/coshocton/coshocton_sample_db.shp')\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "def displace_point(point, max_dist):\n",
    "    angle = np.random.uniform(0, 2 * np.pi)\n",
    "    distance = np.random.uniform(0, max_dist)\n",
    "    new_x = point.x + distance * np.cos(angle)\n",
    "    new_y = point.y + distance * np.sin(angle)\n",
    "    return Point(new_x, new_y)\n",
    "\n",
    "def rp_db(gdf1, buffer_dis, labels, T=100):\n",
    "    precision_all, recall_all, f1_score_all = [], [], []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        for idx, row in gdf1.iterrows():\n",
    "            point = row['geometry']\n",
    "            r = np.max(buffer_dis[idx])\n",
    "            x = displace_point(point, r)\n",
    "            masked_locs.append(x)\n",
    "        gdf = gpd.GeoDataFrame(geometry=masked_locs)\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        db = DBSCAN(eps=500, min_samples=5).fit(points)\n",
    "        preds = db.labels_\n",
    "        preds_binary = [0 if i == -1 else 1 for i in preds]\n",
    "\n",
    "        precision = metrics.precision_score(labels, preds_binary, average='weighted')\n",
    "        recall = metrics.recall_score(labels, preds_binary, average='weighted')\n",
    "        f1_score = metrics.f1_score(labels, preds_binary, average='weighted')\n",
    "        precision_all.append(precision)\n",
    "        recall_all.append(recall)\n",
    "        f1_score_all.append(f1_score)\n",
    "    return sum(precision_all) / len(precision_all), sum(recall_all) / len(recall_all), sum(f1_score_all) / len(f1_score_all)\n",
    "\n",
    "    \n",
    "def lw_db(gdf2, buffer, labels, T=100):\n",
    "    precision_all, recall_all, f1_score_all = [], [], []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        for locs in buffer:\n",
    "            locs = np.delete(locs, 0)\n",
    "            x = random.choice(locs)\n",
    "            masked_locs.append(x)\n",
    "        gdf = gdf2.iloc[masked_locs]\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        db = DBSCAN(eps=500, min_samples=5).fit(points)\n",
    "        preds = db.labels_\n",
    "        preds_binary = [0 if i == -1 else 1 for i in preds]\n",
    "\n",
    "        precision = metrics.precision_score(labels, preds_binary, average='weighted')\n",
    "        recall = metrics.recall_score(labels, preds_binary, average='weighted')\n",
    "        f1_score = metrics.f1_score(labels, preds_binary, average='weighted')\n",
    "        precision_all.append(precision)\n",
    "        recall_all.append(recall)\n",
    "        f1_score_all.append(f1_score)\n",
    "    return sum(precision_all) / len(precision_all), sum(recall_all) / len(recall_all), sum(f1_score_all) / len(f1_score_all)\n",
    "\n",
    "\n",
    "def gm_db(gdf2, prob, buffer, labels, T=100):\n",
    "    precision_all, recall_all, f1_score_all = [], [], []\n",
    "    for t in range(T):\n",
    "        masked_locs = []\n",
    "        labels_all = []\n",
    "        for idx, row in prob.iterrows():\n",
    "            labels_all.append(labels[idx])\n",
    "            locs = buffer[idx]\n",
    "            locs = np.delete(locs, 0)\n",
    "            x = random.choices(locs, weights=tuple(row.tolist()), k=1)[0]\n",
    "            masked_locs.append(x)\n",
    "        gdf = gdf2.iloc[masked_locs]\n",
    "        points = np.array(np.stack([gdf.geometry.x, gdf.geometry.y], axis=1))\n",
    "        db = DBSCAN(eps=500, min_samples=5).fit(points)\n",
    "        preds = db.labels_\n",
    "        preds_binary = [0 if i == -1 else 1 for i in preds]\n",
    "\n",
    "        precision = metrics.precision_score(labels_all, preds_binary, average='weighted')\n",
    "        recall = metrics.recall_score(labels_all, preds_binary, average='weighted')\n",
    "        f1_score = metrics.f1_score(labels_all, preds_binary, average='weighted')\n",
    "        precision_all.append(precision)\n",
    "        recall_all.append(recall)\n",
    "        f1_score_all.append(f1_score)\n",
    "    return sum(precision_all) / len(precision_all), sum(recall_all) / len(recall_all), sum(f1_score_all) / len(f1_score_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_rp: 0.9884320134056394 0.9882633053221285 0.9882461926088046\n",
      "db_lw: 0.9880769983015676 0.987787114845938 0.9878167698781184\n",
      "db_gm: 0.9968143162244039 0.9968000000000008 0.9967927850945298\n",
      "db_gm: 0.9951565062847998 0.9951260504201671 0.9951069723128857\n",
      "db_gm: 0.990436924966586 0.9901680672268904 0.9902117475690211\n",
      "db_gm: 0.9868127199033816 0.9864985994397754 0.9865052599546565\n",
      "db_rp: 0.9848535797558776 0.9846218487394955 0.9846103390290745\n",
      "db_lw: 0.9826710310975416 0.9823809523809517 0.9823813154230433\n",
      "db_gm: 0.9961817525769284 0.9961538461538474 0.9961458749899924\n",
      "db_gm: 0.9898339596853358 0.9897478991596637 0.9896669623957848\n",
      "db_gm: 0.9834221956071099 0.9832212885154058 0.9831950190072121\n",
      "db_gm: 0.9812209441163177 0.9809523809523804 0.9809227823955338\n",
      "db_rp: 0.9791982135214221 0.9788795518207282 0.978856017281703\n",
      "db_lw: 0.9766364974822404 0.9760504201680666 0.9760859159001087\n",
      "db_gm: 0.9953602672245492 0.9953198653198654 0.9953054314944795\n",
      "db_gm: 0.9864024736598406 0.9862184873949578 0.986198344281554\n",
      "db_gm: 0.9849436739964069 0.9848459383753496 0.9848121942653252\n",
      "db_gm: 0.9734839520955861 0.9728011204481785 0.9729007956375803\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "X = np.array(np.stack([gdf1.geometry.x, gdf1.geometry.y], axis=1))\n",
    "db = DBSCAN(eps=500, min_samples=5).fit(X)\n",
    "labels = db.labels_\n",
    "labels_binary = [0 if i == -1 else 1 for i in labels]\n",
    "\n",
    "k_neighbors_all = [10, 20, 30]\n",
    "eps_all = [0.1, 0.01, 0.001, 0.0001]\n",
    "T = 100\n",
    "\n",
    "with open('data/coshocton/sols/coshocton_db_all.csv', 'w') as fw:\n",
    "    fw.write('method,eps,k,precision,recall,f1\\n')\n",
    "    fw.flush()\n",
    "    \n",
    "    for k_neighbors in k_neighbors_all:\n",
    "        buffer = pickle.load(open('data/coshocton/coshocton_buff_k' + str(k_neighbors) + '.pickle', \"rb\"))\n",
    "        buffer_dis = pickle.load(open('data/coshocton/coshocton_buff_dis_k' + str(k_neighbors) + '.pickle', \"rb\"))\n",
    "        buffer_dis = np.delete(buffer_dis, 0, 1)\n",
    "\n",
    "        # rp\n",
    "        db_rp = rp_db(gdf1, buffer_dis, labels_binary, T)\n",
    "        fw.write('rp,,' + str(k_neighbors) + ',' + str(db_rp[0]) + ',' + str(db_rp[1]) + ',' + str(db_rp[2]) + '\\n')\n",
    "        print('db_rp:', db_rp[0], db_rp[1], db_rp[2])\n",
    "        \n",
    "        # lw\n",
    "        db_lw = lw_db(gdf2, buffer, labels_binary, T)\n",
    "        fw.write('lw,,' + str(k_neighbors) + ',' + str(db_lw[0]) + ',' + str(db_lw[1]) + ',' + str(db_lw[2]) + '\\n')\n",
    "        print('db_lw:', db_lw[0], db_lw[1], db_lw[2])\n",
    "        \n",
    "        # gm\n",
    "        for eps in eps_all:\n",
    "            prob = pd.read_csv('data/coshocton/sols/coshocton_prob_eps' + str(eps) + \"_k\" + str(k_neighbors) + '.csv', header=None, index_col=0)\n",
    "            db_gm = gm_db(gdf2, prob, buffer, labels_binary, T)\n",
    "            fw.write('gm,' + str(eps) + ',' + str(k_neighbors) + ',' + str(db_gm[0]) + ',' + str(db_gm[1]) + ',' + str(db_gm[2]) + '\\n')\n",
    "            print('db_gm:', db_gm[0], db_gm[1], db_gm[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
